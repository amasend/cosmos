{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ],
      "text/vnd.plotly.v1+html": [
       "<script>requirejs.config({paths: { 'plotly': ['https://cdn.plot.ly/plotly-latest.min']},});if(!window.Plotly) {{require(['plotly'],function(plotly) {window.Plotly=plotly;});}}</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing.imputation import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import plotly.offline as py\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstepne przetwarzanie danych\n",
    "Z racji, ze jest okolo 19GB danych testowych musialem podzielic je na mniejsze party \n",
    "(moje 32GB RAMu nie sa w stanie uciagnac wszystkiego jesli sie uwzgledni prace pythona,\n",
    "ktory robi przewaznie kopie DataFramow do pamieci).  \n",
    "Inny sposob to wczytywanie batchow do pamieci i operacja na nich bez uzycia posredniego zapisywania na dysku.\n",
    "Jednak jezeli zastosuje sie opcje jaka jest ponizej, tylko pierwsze uruchomienie wymaga dwoch nastepnych komorek. Pozniejsze uruchomienia beda czytac juz z plikow \"test_nr.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\test_set.csv\", \n",
    "                      dtype={\"object_id\":np.int64, \"mjd\":np.float64, \"passband\":np.int64,\n",
    "                             \"flux\":np.float64, \"flux_err\":np.float64, \"detected\":np.int64})\n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podzial na 10 partow (text_n_part.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(set(dataset[\"object_id\"]))\n",
    "n_parts = 10\n",
    "dlugosc =  np.int(np.ceil(len(x)/n_parts))\n",
    "for i, n in zip(range(0, len(x), dlugosc), range(1, 11)):\n",
    "    start = time.time()\n",
    "    dataset[dataset[\"object_id\"].isin(x[i:i+dlugosc])].to_csv(\"..\\\\test_{}.csv\".format(n))\n",
    "    end = time.time()\n",
    "    print(\"test_{}.csv saved - elapsed time: {}\".format(n, end-start))\n",
    "%xdel dataset\n",
    "%xdel x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele ML i statystyczne\n",
    "Ogolnie czesc, ktora laduje modele z dysku o ile je mamy zapisane.\n",
    "Jezeli nie, to na podstawie danych treningowych uzcy model i zapisuje na dysk.\n",
    "\n",
    "Zawarty jest tez tutaj model wykrywania outlayers (wartosci odstajacych), w naszym przypadku novelty.\n",
    "Pozniej bedzie potrzebny do klasyfikacji probek, ktore nie pasuja do schematu treningowego \n",
    "i nie powinny miec zadnej z wczesniejszych class/labels. Z racji tego, ze sa lepsze metody wykrywania dodatkowej klasy na ten moment zakomentowane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training dataset loaded\n",
      "training metadataset loaded\n",
      "Class names for training metadataset changed.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "New features computed and added to the train dataset.\n",
      "New feature \"in_our_galaxy\" added.\n",
      "\"distmod\" feature filled with zeros (our galaxy)\n",
      "\"hostgal_specz\" feature removed.\n",
      "Training dataset merged.\n",
      "X and Y values prepared for training sequence.\n",
      "Scaling performed, XGB model trained. Elapsed time: 37.87476086616516\n",
      "Prediction model saved.\n",
      "IsolationForest trained. Elapsed time: 1.4502766132354736\n",
      "Outlayer (IsolationForest) estimator model saved.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "try:\n",
    "    # load the model from disk\n",
    "    filename = '..\\\\LGBM_plus_XGB_model_log_loss.sav'\n",
    "    clf = pickle.load(open(filename, 'rb'))\n",
    "    print(\"Prediction model loaded.\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#     # load the model from disk IsolationForest\n",
    "#     filename = '..\\\\ISF.sav'\n",
    "#     isf = pickle.load(open(filename, 'rb'))\n",
    "#     print(\"Outlayer model loaded\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # load data ... (to be described)\n",
    "    dataset = pd.read_csv(\"C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\training_set.csv\")\n",
    "    print(\"training dataset loaded\")\n",
    "    # load metadata, ... (to be described)\n",
    "    meta_dataset = pd.read_csv('C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\training_set_metadata.csv')\n",
    "    print(\"training metadataset loaded\")\n",
    "    column_names = {6: \"class_6\", 15: \"class_15\", 16: \"class_16\", 42: \"class_42\", 52: \"class_52\", 53: \"class_53\",\n",
    "                    62: \"class_62\", 64: \"class_64\", 65: \"class_65\", 67: \"class_67\", 88: \"class_88\", 90: \"class_90\",\n",
    "                    92: \"class_92\", 95: \"class_95\"}\n",
    "    # change labels according to sample submission example\n",
    "    meta_dataset[\"target\"] = list(map(lambda name: column_names[name], meta_dataset[\"target\"]))\n",
    "    print(\"Class names for training metadataset changed.\")\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# analysis part\n",
    "    for key, val in {'u': 0, 'g': 1, 'r': 2, 'i': 3, 'z': 4, 'y': 5}.items():\n",
    "        dataset[key] = (dataset['passband']-val).apply(np.bool).apply(np.logical_not).apply(np.int)\n",
    "    print(\"Particular passbands extraction completed.\")\n",
    "    # convert mjd to unix time\n",
    "    unix_time = (dataset['mjd']-40587)*86400\n",
    "    #convert unix time to readable format\n",
    "    time_ = pd.to_datetime(unix_time, unit='ms')\n",
    "    dataset[\"hour\"] = time_.dt.hour\n",
    "    dataset[\"minute\"] = time_.dt.minute\n",
    "    dataset[\"second\"] = time_.dt.second\n",
    "    dataset[\"ms\"] = time_.dt.microsecond\n",
    "    print(\"MJD column decomposed to hour, minute, second and ms columns\")\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    dataset['flux_ratio_sq'] = np.power(dataset['flux'] / dataset['flux_err'], 2.0)\n",
    "    dataset['flux_by_flux_ratio_sq'] = dataset['flux'] * dataset['flux_ratio_sq']\n",
    "\n",
    "    aggs = {\n",
    "        'mjd': ['min', 'max', 'size'],\n",
    "        'passband': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum','skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "        'r': ['sum'],\n",
    "        'g': ['sum'],\n",
    "        'i': ['sum'],\n",
    "        'u': ['sum'],\n",
    "        'z': ['sum'],\n",
    "        'y': ['sum'],\n",
    "        'hour': ['min', 'max', 'median'],\n",
    "        'minute': ['min', 'max', 'median'],\n",
    "        'second': ['min', 'max', 'median'],\n",
    "        'ms': ['min', 'max', 'median'],\n",
    "    }\n",
    "\n",
    "    agg_train = dataset.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_train.columns = new_columns\n",
    "    agg_train['hour_diff'] = agg_train['hour_max'] - agg_train['hour_min']\n",
    "    agg_train['minute_diff'] = agg_train['minute_max'] - agg_train['minute_min']\n",
    "    agg_train['second_diff'] = agg_train['second_max'] - agg_train['second_min']\n",
    "    agg_train['ms_diff'] = agg_train['ms_max'] - agg_train['ms_min']\n",
    "    agg_train['mjd_diff'] = agg_train['mjd_max'] - agg_train['mjd_min']\n",
    "    agg_train['flux_diff'] = agg_train['flux_max'] - agg_train['flux_min']\n",
    "    agg_train['flux_dif2'] = (agg_train['flux_max'] - agg_train['flux_min']) / agg_train['flux_mean']\n",
    "    agg_train['flux_w_mean'] = agg_train['flux_by_flux_ratio_sq_sum'] / agg_train['flux_ratio_sq_sum']\n",
    "    agg_train['flux_dif3'] = (agg_train['flux_max'] - agg_train['flux_min']) / agg_train['flux_w_mean']\n",
    "\n",
    "    del agg_train['mjd_max'], agg_train['mjd_min']\n",
    "    del agg_train['hour_max'], agg_train['hour_min']\n",
    "    del agg_train['minute_max'], agg_train['minute_min']\n",
    "    del agg_train['second_max'], agg_train['second_min']\n",
    "    del agg_train['ms_max'], agg_train['ms_min']\n",
    "    agg_train.head()\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "    print(\"New features computed and added to the train dataset.\")\n",
    "    meta_dataset['in_our_galaxy'] = meta_dataset['distmod'].apply(np.isnan).astype(int)\n",
    "    print(\"New feature \\\"in_our_galaxy\\\" added.\")\n",
    "    meta_dataset = meta_dataset.fillna(0)\n",
    "    print(\"\\\"distmod\\\" feature filled with zeros (our galaxy)\")\n",
    "    meta_dataset = meta_dataset.drop(['hostgal_specz'], axis=1)\n",
    "    print(\"\\\"hostgal_specz\\\" feature removed.\")\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    training_dataset = pd.merge(agg_train.reset_index(), meta_dataset)\n",
    "    print(\"Training dataset merged.\")\n",
    "    \n",
    "# w tym miejscu konczy sie preparowanie danych do uczenia modelu\n",
    "\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    # train model and show score\n",
    "    Y = training_dataset['target'] #wszystkie labels (gotowe do wrzucenia do modelu)\n",
    "    X = training_dataset.drop(['target', 'object_id'], axis=1) #wszystkie dane treningowe (gotowe do wrzucenia do modelu)\n",
    "    print(\"X and Y values prepared for training sequence.\")\n",
    "    start = time.time()\n",
    "    models = [(\"XGBClassifier\", XGBClassifier()), (\"LGBMClassifier\", LGBMClassifier())]\n",
    "    model = VotingClassifier(models, voting='soft')\n",
    "    clf = make_pipeline(StandardScaler(), model)\n",
    "    clf.fit(X,Y)\n",
    "    end = time.time()\n",
    "    print(\"Scaling performed, XGB and LGBM model trained. Elapsed time: {}\".format(end-start))\n",
    "\n",
    "    # save model to disc\n",
    "    filename = '..\\\\LGBM_plus_XGB_model_log_loss.sav'\n",
    "    pickle.dump(clf, open(filename, 'wb'))\n",
    "    print(\"Prediction model saved.\")\n",
    "    \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# IsolationForest part (novelty detection)\n",
    "    # fit novelty detector model IsolationForest (~100 seconds computing time)\n",
    "#     start = time.time()\n",
    "#     isf = make_pipeline(StandardScaler(), IsolationForest())\n",
    "#     isf.fit(np.ascontiguousarray(X.values\n",
    "#     end = time.time()\n",
    "#     print(\"IsolationForest trained. Elapsed time: {}\".format(end-start))\n",
    "    \n",
    "#     # save isf model\n",
    "#     filename = '..\\\\ISF.sav'\n",
    "#     pickle.dump(isf, open(filename, 'wb'))\n",
    "#     print(\"Outlayer (IsolationForest) estimator model saved.\")\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict part\n",
    "Klasyfikacja probek, plus utworzenie nowej klasy w zaleznosci czy probka jest odstajaca czy nie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NameError: name 'dataset' is not defined\n",
      "NameError: name 'imputed_meta_dataset' is not defined\n",
      "NameError: name 'X_train' is not defined\n",
      "NameError: name 'X_test' is not defined\n",
      "NameError: name 'Y_train' is not defined\n",
      "NameError: name 'Y_test' is not defined\n",
      "After memory relocation\n",
      "Test metadataset loaded.\n",
      "All missing values filled with zeros (distmod)\n",
      "\"hostgal_specz\" feature removed.\n",
      "New feature \"in_our_galaxy\" added.\n",
      "Part 1 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 1 Predict Elapsed time: 45.36005759239197\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 38.096203327178955\n",
      "IsolationForest finished. Elapsed time: 84.00627517700195\n",
      "Save predictions to file started...\n",
      "Part 1 saved to CSV. Elapsed time: 17.228618144989014\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 2 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 2 Predict Elapsed time: 37.282268047332764\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 33.89558410644531\n",
      "IsolationForest finished. Elapsed time: 71.6456024646759\n",
      "Save predictions to file started...\n",
      "Part 2 saved to CSV. Elapsed time: 13.625548362731934\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 3 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 3 Predict Elapsed time: 34.38378977775574\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 31.87689757347107\n",
      "IsolationForest finished. Elapsed time: 66.71321320533752\n",
      "Save predictions to file started...\n",
      "Part 3 saved to CSV. Elapsed time: 16.614912033081055\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 4 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 4 Predict Elapsed time: 35.57925987243652\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 31.712979316711426\n",
      "IsolationForest finished. Elapsed time: 67.742516040802\n",
      "Save predictions to file started...\n",
      "Part 4 saved to CSV. Elapsed time: 13.148363828659058\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 5 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 5 Predict Elapsed time: 38.88180112838745\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 36.35227084159851\n",
      "IsolationForest finished. Elapsed time: 75.75600814819336\n",
      "Save predictions to file started...\n",
      "Part 5 saved to CSV. Elapsed time: 15.360318183898926\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 6 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 6 Predict Elapsed time: 34.67764377593994\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 31.967916250228882\n",
      "IsolationForest finished. Elapsed time: 67.1141939163208\n",
      "Save predictions to file started...\n",
      "Part 6 saved to CSV. Elapsed time: 13.032163143157959\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 7 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 7 Predict Elapsed time: 35.82822895050049\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 32.14109420776367\n",
      "IsolationForest finished. Elapsed time: 68.42233896255493\n",
      "Save predictions to file started...\n",
      "Part 7 saved to CSV. Elapsed time: 13.295843601226807\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 8 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 8 Predict Elapsed time: 41.49218034744263\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 33.62566041946411\n",
      "IsolationForest finished. Elapsed time: 75.6564428806305\n",
      "Save predictions to file started...\n",
      "Part 8 saved to CSV. Elapsed time: 15.952642440795898\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 9 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 9 Predict Elapsed time: 41.734906911849976\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 37.50033640861511\n",
      "IsolationForest finished. Elapsed time: 79.766366481781\n",
      "Save predictions to file started...\n",
      "Part 9 saved to CSV. Elapsed time: 16.42143154144287\n",
      "NameError: name 'dataset_transformed' is not defined\n",
      "Part 10 test dataset loaded.\n",
      "Particular passbands extraction completed.\n",
      "MJD column decomposed to hour, minute, second and ms columns\n",
      "Test dataset merged.\n",
      "NameError: name 'dataset' is not defined\n",
      "Prediction started...\n",
      "Part 10 Predict Elapsed time: 44.53356909751892\n",
      "IsolationForest prediction started...\n",
      "After search, elapsed time: 37.63459134101868\n",
      "IsolationForest finished. Elapsed time: 82.64613890647888\n",
      "Save predictions to file started...\n",
      "Part 10 saved to CSV. Elapsed time: 18.73462963104248\n",
      "NameError: name 'dataset_transformed' is not defined\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# def predictions(predicted_dataframe, object_id):\n",
    "#     # ta metoda jak na razie spisywala sie najlepiej (nie wyprobowalem jej z lepszymi klasyfikatorami, warto sprobowac)\n",
    "#     \"\"\"Search rows with low prediction accuracy, if none of class was predicted higher then 0.6, classify as outlayer.\n",
    "#     Add class_99 column for outlayers\"\"\"\n",
    "#     columns = predicted_dataframe.columns.values.tolist()\n",
    "#     start = time.time()\n",
    "#     class_99 = np.any(predicted_dataframe[predicted_dataframe[columns] <= 0.6].apply(np.isnan), axis=1).apply(np.logical_not).apply(np.int)\n",
    "#     predicted_dataframe[\"class_99\"] = class_99\n",
    "#     end = time.time()\n",
    "#     print(\"After search, elapsed time: {}\".format(end-start))\n",
    "#     predicted_dataframe.insert(0,\"object_id\",object_id)\n",
    "    \n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# Isolation Forest part\n",
    "# def predictions_ISF(predicted_dataframe, object_id, data, isf):\n",
    "#     \"\"\"Use IsolationForest to classify if there is an outlayer, unseen label/class\"\"\"\n",
    "#     columns = predicted_dataframe.columns.values.tolist()\n",
    "#     n_samples = 1000000\n",
    "#     l_data = len(data)\n",
    "    \n",
    "#     start = time.time()\n",
    "#     class_99 =  np.array([])\n",
    "#     for i in range(n_samples, l_data, n_samples): # IsolationForest cannot handle big datasets (32GB of RAM allocation is not enough)\n",
    "#         #http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html#sklearn.ensemble.IsolationForest.predict\n",
    "#         class_99 = np.append(class_99, isf.predict(data[i-n_samples:i])) # IsolationForest predicts if sample is outlayer (-1 or inlayer 1)\n",
    "#         print(\"Batch {} of {}, elapsed time: {}\".format(i/n_samples, round(l_data/n_samples), time.time()-start))\n",
    "#     class_99 = np.append(class_99, isf.predict(data[l_data-l_data%n_samples:]))\n",
    "#     class_99[class_99 == 1] = 0 # change inlayers to 0 value\n",
    "#     predicted_dataframe[\"class_99\"] = -1*class_99 # append predictions with new class column\n",
    "#     end = time.time()\n",
    "    \n",
    "#     print(\"After search, elapsed time: {}\".format(end-start))\n",
    "#     predicted_dataframe.insert(0,\"object_id\",object_id)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "\n",
    "def predictions_engi(predicted_dataframe, object_id, data):\n",
    "    start = time.time()\n",
    "    class_99 = np.ones(data.shape[0])\n",
    "    for i in range(data.shape[1]):\n",
    "        class_99 *= (1-data.iloc[:,i])\n",
    "    predicted_dataframe['class_99'] = 0.14*class_99/np.mean(class_99)\n",
    "    end = time.time()\n",
    "    print(\"After search, elapsed time: {}\".format(end-start))\n",
    "    predicted_dataframe.insert(0,\"object_id\",object_id)\n",
    "    \n",
    "%xdel dataset\n",
    "%xdel meta_dataset\n",
    "%xdel imputed_meta_dataset\n",
    "%xdel training_dataset\n",
    "%xdel X_train\n",
    "%xdel X_test\n",
    "%xdel Y_train\n",
    "%xdel Y_test\n",
    "print(\"After memory relocation\")\n",
    "\n",
    "meta_dataset = pd.read_csv('C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\test_set_metadata.csv')\n",
    "print(\"Test metadataset loaded.\")\n",
    "meta_dataset = meta_dataset.fillna(0)\n",
    "print(\"All missing values filled with zeros (distmod)\")\n",
    "meta_dataset = meta_dataset.drop(['hostgal_specz'], axis=1)\n",
    "print(\"\\\"hostgal_specz\\\" feature removed.\")\n",
    "meta_dataset['in_our_galaxy'] = meta_dataset['distmod'].apply(np.isnan).astype(int)\n",
    "print(\"New feature \\\"in_our_galaxy\\\" added.\")\n",
    "\n",
    "\n",
    "for i in range(1,11):\n",
    "    dataset = pd.read_csv(\"..\\\\test_{}.csv\".format(i), index_col=0)\n",
    "    print(\"Part {} test dataset loaded.\".format(i))\n",
    "    \n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# analysis part\n",
    "    for key, val in {'u': 0, 'g': 1, 'r': 2, 'i': 3, 'z': 4, 'y': 5}.items():\n",
    "        dataset[key] = (dataset['passband']-val).apply(np.bool).apply(np.logical_not).apply(np.int)\n",
    "    print(\"Particular passbands extraction completed.\")\n",
    "    # convert mjd to unix time\n",
    "    unix_time = (dataset['mjd']-40587)*86400\n",
    "    #convert unix time to readable format\n",
    "    time_ = pd.to_datetime(unix_time, unit='ms')\n",
    "    dataset[\"hour\"] = time_.dt.hour\n",
    "    dataset[\"minute\"] = time_.dt.minute\n",
    "    dataset[\"second\"] = time_.dt.second\n",
    "    dataset[\"ms\"] = time_.dt.microsecond\n",
    "    print(\"MJD column decomposed to hour, minute, second and ms columns\")\n",
    "    import gc\n",
    "    gc.enable()\n",
    "    dataset['flux_ratio_sq'] = np.power(dataset['flux'] / dataset['flux_err'], 2.0)\n",
    "    dataset['flux_by_flux_ratio_sq'] = dataset['flux'] * dataset['flux_ratio_sq']\n",
    "\n",
    "    aggs = {\n",
    "        'mjd': ['min', 'max', 'size'],\n",
    "        'passband': ['min', 'max', 'mean', 'median', 'std'],\n",
    "        'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "        'detected': ['mean'],\n",
    "        'flux_ratio_sq':['sum','skew'],\n",
    "        'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "        'r': ['sum'],\n",
    "        'g': ['sum'],\n",
    "        'i': ['sum'],\n",
    "        'u': ['sum'],\n",
    "        'z': ['sum'],\n",
    "        'y': ['sum'],\n",
    "        'hour': ['min', 'max', 'median'],\n",
    "        'minute': ['min', 'max', 'median'],\n",
    "        'second': ['min', 'max', 'median'],\n",
    "        'ms': ['min', 'max', 'median'],\n",
    "    }\n",
    "\n",
    "    agg_train = dataset.groupby('object_id').agg(aggs)\n",
    "    new_columns = [k + '_' + agg for k in aggs.keys() for agg in aggs[k]]\n",
    "    agg_train.columns = new_columns\n",
    "    agg_train['hour_diff'] = agg_train['hour_max'] - agg_train['hour_min']\n",
    "    agg_train['minute_diff'] = agg_train['minute_max'] - agg_train['minute_min']\n",
    "    agg_train['second_diff'] = agg_train['second_max'] - agg_train['second_min']\n",
    "    agg_train['ms_diff'] = agg_train['ms_max'] - agg_train['ms_min']\n",
    "    agg_train['mjd_diff'] = agg_train['mjd_max'] - agg_train['mjd_min']\n",
    "    agg_train['flux_diff'] = agg_train['flux_max'] - agg_train['flux_min']\n",
    "    agg_train['flux_dif2'] = (agg_train['flux_max'] - agg_train['flux_min']) / agg_train['flux_mean']\n",
    "    agg_train['flux_w_mean'] = agg_train['flux_by_flux_ratio_sq_sum'] / agg_train['flux_ratio_sq_sum']\n",
    "    agg_train['flux_dif3'] = (agg_train['flux_max'] - agg_train['flux_min']) / agg_train['flux_w_mean']\n",
    "\n",
    "    del agg_train['mjd_max'], agg_train['mjd_min']\n",
    "    del agg_train['hour_max'], agg_train['hour_min']\n",
    "    del agg_train['minute_max'], agg_train['minute_min']\n",
    "    del agg_train['second_max'], agg_train['second_min']\n",
    "    del agg_train['ms_max'], agg_train['ms_min']\n",
    "    agg_train.head()\n",
    "\n",
    "    del dataset\n",
    "    gc.collect()\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    \n",
    "    dataset_merged = pd.merge(agg_train.reset_index(), meta_dataset)\n",
    "    print(\"Test dataset merged.\")\n",
    "    %xdel dataset\n",
    "# tutaj konczy sie czesc z przygotowaniem danych do predykcji (testowych)\n",
    "    object_id = dataset_merged[\"object_id\"]\n",
    "    print(\"Prediction started...\")\n",
    "    start = time.time()\n",
    "    predicted_et = pd.DataFrame(clf.predict_proba(dataset_merged.iloc[:,1:].values), columns=clf.classes_)\n",
    "    end = time.time()\n",
    "    print(\"Part {} Predict Elapsed time: {}\".format(i, end-start))\n",
    "    \n",
    "    # IsolationForest method\n",
    "    print(\"Additional class prediction started...\")\n",
    "    strt = time.time()\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "#IsolationForest Part\n",
    "#     predictions_ISF(predicted_et, object_id, dataset_merged.iloc[:,1:].values, isf)\n",
    "#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    predictions_engi(predicted_et, object_id, dataset_merged.iloc[:,1:])\n",
    "    end = time.time()\n",
    "    print(\"Additional class prediction finished. Elapsed time: {}\".format(end-start))\n",
    "#     Manual method\n",
    "#     predictions(predicted_et, object_id)\n",
    "    print(\"Save predictions to file started...\")\n",
    "    start = time.time()\n",
    "    predicted_et.to_csv(\"..\\\\predicted_et_n_{}.csv\".format(i))\n",
    "    end = time.time()\n",
    "    print(\"Part {} saved to CSV. Elapsed time: {}\".format(i, end-start))\n",
    "    %xdel dataset_merged\n",
    "    %xdel predicted_et\n",
    "    %xdel dataset_transformed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupowanie\n",
    "Scalanie partow w jedna calosc i zapis na dysk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group 1 added. Elapsed time: 6.890550374984741\n",
      "Group 2 added. Elapsed time: 4.0390825271606445\n",
      "Group 3 added. Elapsed time: 4.497156143188477\n",
      "Group 4 added. Elapsed time: 4.541804075241089\n",
      "Group 5 added. Elapsed time: 4.0001747608184814\n",
      "Group 6 added. Elapsed time: 4.202584981918335\n",
      "Group 7 added. Elapsed time: 3.741884708404541\n",
      "Group 8 added. Elapsed time: 2.593755006790161\n",
      "Group 9 added. Elapsed time: 2.712012529373169\n",
      "Group 10 added. Elapsed time: 2.918860673904419\n"
     ]
    }
   ],
   "source": [
    "# merge all parts into one dataset\n",
    "for i in range(1,11):\n",
    "    start = time.time()\n",
    "    if i == 1:\n",
    "        dataset = pd.read_csv(\"..\\\\predicted_et_n_{}.csv\".format(i), index_col=0)\n",
    "    else:\n",
    "        dataset = pd.concat([dataset, pd.read_csv(\"..\\\\predicted_et_n_{}.csv\".format(i), index_col=0)])\n",
    "    end = time.time()\n",
    "    print(\"Group {} added. Elapsed time: {}\".format(i, end-start))\n",
    "\n",
    "dataset.to_csv(\"..\\\\LSST_LGBM_XGB.csv\", index=False)\n",
    "%xdel dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
