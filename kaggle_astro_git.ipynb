{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ponizej krotki opis co przetestowalem do tej pory z algorytmow ML  \n",
    "Na ta chwile opieram sie na Estra Trees -> dal mi najwyzsza dokladnosc jesli chodzi o accuracy.\n",
    "W sekcji z treningiem modelu wzialem juz pod uwage, ze score jest liczony inaczej niz accuracy, \n",
    "a mianowicie log_loss, co zmienia troche postac rzeczy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "wszystkie probki treningowe - bez tuningu  \n",
    "testowe accuracy: 0.9817191330128261  \n",
    "treningowe time: 44.32341456413269   ---   accuracy: 0.9991453923282256  \n",
    "\n",
    "wszystkie - max_depth= 70, n_estimators= 100  \n",
    "testowe: 0.9848913804199887  \n",
    "treningowe: time: 469.39809942245483   ---   accuracy: 0.9999982415480004  \n",
    "\n",
    "# Extra Trees\n",
    "wszystkie probki treningowe - bez tuningu  \n",
    "testowe accuracy: 0.9822326009966906  \n",
    "treningowe time: 19.31137776374817   ---   accuracy: 1.0  \n",
    "\n",
    "wszystkie - 'max_depth': None, 'n_estimators': 100   \n",
    "testowe: 0.9837343190042941  \n",
    "treningowe: time: 184.10754990577698   ---   1.0  \n",
    "\n",
    "# Ada Boost\n",
    "wszystkie probki treningowe - bez tuningu  \n",
    "testowe accuracy: 0.4523828783045709  \n",
    "treningowe time: 174.7924304008484   ---   0.4537878814522 \n",
    "\n",
    "wszystkie - ??????  \n",
    "testowe: ??????  \n",
    "treningowe: time: ??????   ---   accuracy: ??????\n",
    "\n",
    "# eXtreme Gradient Boosting\n",
    "wszystkie probki treningowe - bez tuningu  \n",
    "testowe accuracy: 0.6213947337879517   \n",
    "treningowe time: 1652.873502254486   ---   0.622957997615539 \n",
    "\n",
    "wszystkie - ??????  \n",
    "testowe: ??????  \n",
    "treningowe: time: ??????   ---   accuracy: ??????\n",
    "\n",
    "# Gradient Boosting\n",
    "wszystkie probki treningowe - bez tuningu  \n",
    "testowe accuracy: 0.6797894077885356   \n",
    "treningowe time: 3958.553186416626   ---   0.6825616073658037\n",
    "\n",
    "wszystkie - ??????  \n",
    "testowe: ??????  \n",
    "treningowe: time: ??????   ---   accuracy: ??????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import time\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib\n",
    "from matplotlib import colors as mcolors\n",
    "from sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing.imputation import Imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wstepne przetwarzanie danych\n",
    "Z racji, ze jest okolo 19GB danych testowych musialem podzielic je na mniejsze party \n",
    "(moje 32GB RAMu nie sa w stanie uciagnac wszystkiego jesli sie uwzgledni prace pythona,\n",
    "ktory robi przewaznie kopie DataFramow do pamieci)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\test_set.csv\", \n",
    "                      dtype={\"object_id\":np.int64, \"mjd\":np.float64, \"passband\":np.int64,\n",
    "                             \"flux\":np.float64, \"flux_err\":np.float64, \"detected\":np.int64})\n",
    "print(\"Dataset loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podzial na 10 partow (text_n_part.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list(set(dataset[\"object_id\"]))\n",
    "n_parts = 10\n",
    "dlugosc =  np.int(np.ceil(len(x)/n_parts))\n",
    "for i, n in zip(range(0, len(x), dlugosc), range(1, 11)):\n",
    "    start = time.time()\n",
    "    dataset[dataset[\"object_id\"].isin(x[i:i+dlugosc])].to_csv(\"..\\\\test_{}.csv\".format(n))\n",
    "    end = time.time()\n",
    "    print(\"test_{}.csv saved - elapsed time: {}\".format(n, end-start))\n",
    "%xdel dataset\n",
    "%xdel x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modele ML i statystyczne\n",
    "Ogolnie czesc, ktora laduje modele z dysku o ile je mamy zapisane.\n",
    "Jezeli nie, to na podstawie danych treningowych uzcy model i zapisuje na dysk.\n",
    "\n",
    "Zawarty jest tez tutaj model wykrywania outlayers (wartosci odstajacych), w naszym przypadku novelty.\n",
    "Pozniej bedzie potrzebny do klasyfikacji probek, ktore nie pasuja do schematu treningowego \n",
    "i nie powinny miec zadnej z wczesniejszych class/labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.metrics import log_loss\n",
    "try:\n",
    "    # load the model from disk\n",
    "    filename = '..\\\\ET_model_log_loss.sav'\n",
    "    et = pickle.load(open(filename, 'rb'))\n",
    "    print(\"Prediction model loaded.\")\n",
    "    \n",
    "    # load the model from disk IsolationForest\n",
    "    filename = '..\\\\ISF.sav'\n",
    "    isf = pickle.load(open(filename, 'rb'))\n",
    "    print(\"Outlayer model loaded\")\n",
    "    \n",
    "#     # load the model from disk LocalOutlayerFactor\n",
    "#     filename = '..\\\\LOF.sav'\n",
    "#     lof = pickle.load(open(filename, 'rb'))\n",
    "#     print(\"Outlayer model saved\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    # load data ... (to be described)\n",
    "    dataset = pd.read_csv(\"C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\training_set.csv\")\n",
    "    # load metadata, ... (to be described)\n",
    "    meta_dataset = pd.read_csv('C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\training_set_metadata.csv')\n",
    "    column_names = {6: \"class_6\", 15: \"class_15\", 16: \"class_16\", 42: \"class_42\", 52: \"class_52\", 53: \"class_53\",\n",
    "                    62: \"class_62\", 64: \"class_64\", 65: \"class_65\", 67: \"class_67\", 88: \"class_88\", 90: \"class_90\",\n",
    "                    92: \"class_92\", 95: \"class_95\"}\n",
    "    # change labels according to sample submission example\n",
    "    meta_dataset[\"target\"] = list(map(lambda name: column_names[name], meta_dataset[\"target\"]))\n",
    "    # use imputer to search for NaN values and compute mean() values instead of NaN (column vise)\n",
    "    mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "    mean_imputer = mean_imputer.fit(meta_dataset.iloc[:,:-1]) # without last target column (imputer does not recognize string data)\n",
    "    imputed_meta_dataset = mean_imputer.transform(meta_dataset.iloc[:,:-1].values)\n",
    "    imputed_meta_dataset = pd.DataFrame(data=imputed_meta_dataset, columns=meta_dataset.iloc[:,:-1].columns.values)\n",
    "    imputed_meta_dataset[\"target\"] = meta_dataset.iloc[:,-1] # add last target column to imputed meta dataset\n",
    "    # change object_id and ddf values back to int() ... imputer interprets all values as float()\n",
    "    imputed_meta_dataset[\"object_id\"] = list(map(lambda val: int(val), imputed_meta_dataset[\"object_id\"]))\n",
    "    imputed_meta_dataset[\"ddf\"] = list(map(lambda val: int(val), imputed_meta_dataset[\"ddf\"]))\n",
    "    # merges two datasets, merge by group_id -> common key in both DataFrames\n",
    "    training_dataset = pd.merge(dataset, imputed_meta_dataset)\n",
    "    # check if training_dataset consists of any empty values\n",
    "    columns_missing = [col for col in training_dataset.columns if training_dataset[col].isnull().any()]\n",
    "    if columns_missing:\n",
    "        print(\"Dataset has missing values in the following columns:\\n{}\".format(columns_missing))\n",
    "    else:\n",
    "        print(\"Dataset do not has any column with empty value.\")\n",
    "    # split data into training and test datasets (X and Y) ... data is randomly chosen\n",
    "    test_size = 0.2\n",
    "    seed = 7\n",
    "    # description\n",
    "    # train_test_split(X,Y,test_size,random_state)\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(training_dataset.iloc[:,:-1], training_dataset.iloc[:,-1], \n",
    "                                                        test_size=test_size, random_state=seed)\n",
    "    X_train = pd.DataFrame(data=X_train, columns=training_dataset.columns.values.tolist()[:-1])\n",
    "    X_test = pd.DataFrame(data=X_test, columns=training_dataset.columns.values.tolist()[:-1])\n",
    "    import time\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    start = time.time()\n",
    "    et = ExtraTreesClassifier(n_estimators=100)\n",
    "    # .iloc[:,1:].values (removes object_id column from computing)\n",
    "    et.fit(X_train.iloc[:,1:].values, Y_train)\n",
    "    end = time.time()\n",
    "    # accuracy score is not valid here ... accuracy means if we predict label correctly it is very good,\n",
    "    # but not enough big predictions for other labels are ignored\n",
    "    print(\"Extra Trees Elapsed Training time: {}   ---   Training accuracy score: {}\".format(end - start, et.score(X_train.iloc[:,1:].values, Y_train)))\n",
    "    start = time.time()\n",
    "    predictions = et.predict(X_test.iloc[:,1:].values)\n",
    "    end = time.time()\n",
    "    print(\"Extra Trees Elapsed Test prediction time: {}   ---   Testing accuracy score: {}\".format(end - start, accuracy_score(Y_test, predictions)))\n",
    "    # compute predictions for each class for each row (sample)\n",
    "    start = time.time()\n",
    "    predicted_et = pd.DataFrame(et.predict_proba(X_test.iloc[:,1:].values), columns=et.classes_)\n",
    "    end = time.time()\n",
    "    print(\"Predict_proba computed in: {}\".format(end-start))\n",
    "    from sklearn.metrics import log_loss\n",
    "    loss = log_loss(y_true=Y_test, y_pred=predicted_et.values, labels=predicted_et.columns.values.tolist())\n",
    "    print(\"Log_loss: {}\".format(loss))\n",
    "    print(\"Precision: {}\".format(np.exp(-loss)))\n",
    "\n",
    "    # save model\n",
    "    filename = '..\\\\ET_model_log_loss.sav'\n",
    "    pickle.dump(et, open(filename, 'wb'))\n",
    "    print(\"Prediction model saved.\")\n",
    "    \n",
    "    # fit novelty detector model IsolationForest (~100 seconds computing time)\n",
    "    start = time.time()\n",
    "    isf = IsolationForest()\n",
    "    isf.fit(np.ascontiguousarray(X_train.iloc[:,1:].values))\n",
    "    end = time.time()\n",
    "    print(\"after fit: {}\".format(end-start))\n",
    "    \n",
    "    # save isf model\n",
    "    filename = '..\\\\ISF.sav'\n",
    "    pickle.dump(isf, open(filename, 'wb'))\n",
    "    print(\"Outlayer estimator model saved.\")\n",
    "    \n",
    "#     # scaling training data only for LocalOutlayerFactor(~1 second computing time)\n",
    "#     start = time.time()\n",
    "#     scaler = StandardScaler()\n",
    "#     X_transformed = scaler.fit_transform(X_train.iloc[:,1:].values)\n",
    "#     end = time.time()\n",
    "#     print(\"after transformation: {}\".format(end-start))\n",
    "    \n",
    "#     # fit novelty detector model LocalOutlayerFactor (~1800 seconds computing time)\n",
    "#     start = time.time()\n",
    "#     lof = LocalOutlierFactor(novelty=True)\n",
    "#     lof.fit(np.ascontiguousarray(X_transformed))\n",
    "#     end = time.time()\n",
    "#     print(\"after fit: {}\".format(end-start))\n",
    "    \n",
    "#     # save isf model\n",
    "#     filename = '..\\\\LOF.sav'\n",
    "#     pickle.dump(lof, open(filename, 'wb'))\n",
    "#     print(\"Outlayer estimator model saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/predict-closed-questions-on-stack-overflow/discussion/2499  \n",
    "Wyjasnienie odnosnie log loss chociaz raczej nie do konca trafne jesli chodzi o ostatnie zdania (log5....itp).\n",
    "Log_loss powinien schodzic jak najblizej wartosci zerowej wtedy prawdopodobienstwo, ze mamy dobre wyniki jest bliskie jedynki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict part\n",
    "Klasyfikacja probek, plus utworzenie nowej klasy w zaleznosci czy probka jest odstajaca czy nie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "def predictions(predicted_dataframe, object_id):\n",
    "    \"\"\"Search rows with low prediction accuracy, if none of class was predicted higher then 0.6, classify as outlayer.\n",
    "    Add class_99 column for outlayers\"\"\"\n",
    "    columns = predicted_dataframe.columns.values.tolist()\n",
    "    start = time.time()\n",
    "    class_99 = np.any(predicted_dataframe[predicted_dataframe[columns] <= 0.6].apply(np.isnan), axis=1).apply(np.logical_not).apply(np.int)\n",
    "    predicted_dataframe[\"class_99\"] = class_99\n",
    "    end = time.time()\n",
    "    print(\"After search, elapsed time: {}\".format(end-start))\n",
    "    predicted_dataframe.insert(0,\"object_id\",object_id)\n",
    "    \n",
    "def predictions_ISF(predicted_dataframe, object_id, data, isf):\n",
    "    \"\"\"Use IsolationForest to classify if there is an outlayer, unseen label/class\"\"\"\n",
    "    columns = predicted_dataframe.columns.values.tolist()\n",
    "    n_samples = 1000000\n",
    "    l_data = len(data)\n",
    "    \n",
    "    start = time.time()\n",
    "    class_99 =  np.array([])\n",
    "    for i in range(n_samples, l_data, n_samples): # IsolationForest cannot handle big datasets (32GB of RAM allocation is not enough)\n",
    "        class_99 = np.append(class_99, isf.predict(data[i-n_samples:i])) # IsolationForest predicts if sample is outlayer (-1 or inlayer 1)\n",
    "        print(\"Batch {} of {}, elapsed time: {}\".format(i/n_samples, round(l_data/n_samples), time.time()-start))\n",
    "    class_99 = np.append(class_99, isf.predict(data[l_data-l_data%n_samples:]))\n",
    "    class_99[class_99 == -1] = 0 # change inlayers to 0 value\n",
    "    predicted_dataframe[\"class_99\"] = class_99 # append predictions with new class column\n",
    "    end = time.time()\n",
    "    \n",
    "    print(\"After search, elapsed time: {}\".format(end-start))\n",
    "    predicted_dataframe.insert(0,\"object_id\",object_id)\n",
    "    \n",
    "%xdel dataset\n",
    "%xdel meta_dataset\n",
    "%xdel imputed_meta_dataset\n",
    "%xdel training_dataset\n",
    "%xdel X_train\n",
    "%xdel X_test\n",
    "%xdel Y_train\n",
    "%xdel Y_test\n",
    "print(\"After memory relocation\")\n",
    "\n",
    "meta_dataset = pd.read_csv('C:\\\\Users\\\\Amadeusz\\\\Downloads\\\\all\\\\test_set_metadata.csv')\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0)\n",
    "mean_imputer = mean_imputer.fit(meta_dataset)\n",
    "imputed_meta_dataset = mean_imputer.transform(meta_dataset.values)\n",
    "imputed_meta_dataset = pd.DataFrame(data=imputed_meta_dataset, columns=meta_dataset.columns.values)\n",
    "# change object_id and ddf values back to int() ... imputer interprets all values as float()\n",
    "imputed_meta_dataset[\"object_id\"] = list(map(lambda val: int(val), imputed_meta_dataset[\"object_id\"]))\n",
    "imputed_meta_dataset[\"ddf\"] = list(map(lambda val: int(val), imputed_meta_dataset[\"ddf\"]))\n",
    "\n",
    "for i in range(1,11):\n",
    "    dataset = pd.read_csv(\"..\\\\test_{}.csv\".format(i), index_col=0)\n",
    "    dataset_merged = pd.merge(dataset, imputed_meta_dataset, on=\"object_id\")\n",
    "    %xdel dataset\n",
    "    object_id = dataset_merged[\"object_id\"]\n",
    "    start = time.time()\n",
    "    predicted_et = pd.DataFrame(et.predict_proba(dataset_merged.iloc[:,1:].values), columns=et.classes_)\n",
    "    end = time.time()\n",
    "    print(\"Part {} Predict Elapsed time: {}\".format(i, end-start))\n",
    "    \n",
    "    # IsolationForest method\n",
    "    predictions_ISF(predicted_et, object_id, dataset_merged.iloc[:,1:].values, isf)\n",
    "    # Manual method\n",
    "#     predictions(predicted_et, object_id)\n",
    "    \n",
    "    start = time.time()\n",
    "    predicted_et.to_csv(\"..\\\\predicted_et_n_{}.csv\".format(i))\n",
    "    end = time.time()\n",
    "    print(\"Part {} Save to CSV Elapsed time: {}\".format(i, end-start))\n",
    "    %xdel dataset_merged\n",
    "    %xdel predicted_et"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grupowanie\n",
    "Test dataset zawiera kilka probek na dany obiekt, \n",
    "z racji tego potrzeba wyestymowac z grupy probek dla obiektu do jakiej klasy nalezy.\n",
    "Najlepiej dotad sprawdzila sie srednia z prawdopodobienstw dla danych klas.\n",
    "Nie mam wyniku jeszcze dla sredniej z uwzglednieniem IsolationForest - postaram sie przeprowadzic obliczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# verified faster version (2.5 times faster then slower version version)\n",
    "# def find_best_row(x):\n",
    "#     \"\"\"Finds row with maximum value in each group and returns that row.\"\"\"\n",
    "#     maximum = 0\n",
    "#     return_row = 0\n",
    "#     for ind, row in x.drop([\"object_id\"], axis=1).iterrows():\n",
    "#         r = np.max(row.values)\n",
    "#         if r > maximum:\n",
    "#             maximum = r\n",
    "#             return_row = row\n",
    "#     return return_row\n",
    "\n",
    "# def find_best_row(x):\n",
    "#     \"\"\"Finds row with maximum value in each group and returns that row.\"\"\"\n",
    "# #     s = time.time()\n",
    "#     maximum = np.max(x.values)\n",
    "# #     print(\"After max search, elapsed: {}\".format(time.time()-s))\n",
    "# #     s = time.time()\n",
    "#     z = x[x[x.columns] == maximum].fillna(0)\n",
    "# #     print(\"After fillna, elapsed: {}\".format(time.time()-s))\n",
    "#     return_row = 0\n",
    "# #     s = time.time()\n",
    "#     for ind, row in z.iterrows():\n",
    "#         r = np.max(row.values)\n",
    "#         if r == maximum:\n",
    "#             return_row = row\n",
    "#             break\n",
    "# #     print(\"After for loop, elapsed: {}\".format(time.time()-s))\n",
    "#     return return_row  # leave only maximum prediction for row, other fill with zero\n",
    "\n",
    "# slower version\n",
    "# def find_best_row(x):\n",
    "#     \"\"\"Finds row with maximum value in each group and returns that row.\"\"\"\n",
    "#     index = x.drop([\"object_id\"], axis=1).T.max().idxmax()\n",
    "#     return x.loc[[index]]\n",
    "\n",
    "# compute grouped results for each object_id\n",
    "for i in range(1,11):\n",
    "    start = time.time()\n",
    "    dataset = pd.read_csv(\"..\\\\predicted_et_n_{}.csv\".format(i), index_col=0)\n",
    "    print(\"predicted_et loaded, time elapsed: {}\".format(time.time()-start))\n",
    "#     grouped = dataset.groupby(by=\"object_id\").apply(lambda x: find_best_row(x)).reset_index(level=1, drop=True).drop([\"object_id\"], axis=1)\n",
    "#     print(\"Dataset grouped and best row found. time elapsed: {}\".format(time.time() - start))\n",
    "#     grouped = dataset.groupby(by=\"object_id\").apply(lambda x: find_best_row(x.drop([\"object_id\"], axis=1)))    # returns only best row (for loop)\n",
    "#     print(\"Dataset grouped and best row found. time elapsed: {}\".format(time.time() - start))\n",
    "    grouped = dataset.groupby(by=\"object_id\").mean()    # returns mean of grouped rows\n",
    "    grouped.to_csv(\"..\\\\grouped_{}.csv\".format(i))\n",
    "    %xdel grouped\n",
    "    %xdel dataset\n",
    "    end = time.time()\n",
    "    print(\"Group {} saved. Elapsed time: {}\".format(i, end-start))\n",
    "\n",
    "# merge all parts into one dataset\n",
    "for i in range(1,11):\n",
    "    start = time.time()\n",
    "    if i == 1:\n",
    "        dataset = pd.read_csv(\"..\\\\grouped_{}.csv\".format(i), index_col=0)\n",
    "    else:\n",
    "        dataset = pd.concat([dataset, pd.read_csv(\"..\\\\grouped_{}.csv\".format(i))], ignore_index=True)\n",
    "    end = time.time()\n",
    "    print(\"Group {} added. Elapsed time: {}\".format(i, end-start))\n",
    "\n",
    "dataset.to_csv(\"..\\\\LSST_project_prediction_mean_ISF.csv\", index=False)\n",
    "%xdel dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
